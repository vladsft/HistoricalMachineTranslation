{"cells":[{"cell_type":"markdown","metadata":{"id":"viJIdP051cbg"},"source":["Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4929,"status":"ok","timestamp":1656087466364,"user":{"displayName":"Vlad Safta","userId":"06171379063017095917"},"user_tz":-60},"id":"CJEPGps2xnsN"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","import pickle\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","import random\n","import unicodedata\n","import re\n","import torch.cuda\n","\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import make_regression\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","\n","from scipy.spatial import distance"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1656087532301,"user":{"displayName":"Vlad Safta","userId":"06171379063017095917"},"user_tz":-60},"id":"WjCjN2Y78Wy1","outputId":"51f4259f-82f5-46e6-e0b1-b313bbb26948"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/IndividualProject\n"]}],"source":["%cd drive/MyDrive/IndividualProject/"]},{"cell_type":"markdown","metadata":{"id":"0hxPZoxy1gEl"},"source":["# Getting the Translation Files"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":97515,"status":"ok","timestamp":1656087822317,"user":{"displayName":"Vlad Safta","userId":"06171379063017095917"},"user_tz":-60},"id":"TLxMHqAS1mTc"},"outputs":[],"source":["en_vocabs = []\n","fr_vocabs = []\n","de_vocabs = []\n","chi_vocabs = []\n","en_weights = []\n","fr_weights = []\n","de_weights = []\n","chi_weights = []\n","\n","for year in range(1800, 2000, 10):\n","  path = \"ProjectSoftwareArchive/\"\n","  vocab_filename = str(year) + \"-vocab.pkl\";\n","  weights_filename = str(year) + \"-w.npy\"\n","  en_vocabs.append(pickle.load(open(path + \"en-embs/\" + vocab_filename, 'rb')))\n","  fr_vocabs.append(pickle.load(open(path + \"fr-embs/\" + vocab_filename, 'rb')))\n","  de_vocabs.append(pickle.load(open(path + \"de-embs/\" + vocab_filename, 'rb')))\n","  en_weights.append(np.load(path + \"en-embs/\" + weights_filename))\n","  fr_weights.append(np.load(path + \"fr-embs/\" + weights_filename))\n","  de_weights.append(np.load(path + \"de-embs/\" + weights_filename))\n","  if (year >= 1950):\n","    chi_vocabs.append(pickle.load(open(path + \"chi-embs/\" + vocab_filename, 'rb')))\n","    chi_weights.append(np.load(path + \"chi-embs/\" + weights_filename))\n","  else:\n","    chi_vocabs.append([0])\n","    chi_weights.append([0])\n","\n","# vocabs array : 0 for English, 1 for French, 2 for German, 3 for Chinese\n","# same Hash Map keys are for the weights\n","vocabs = []\n","weights = []\n","vocabs.append(en_vocabs)\n","vocabs.append(fr_vocabs)\n","vocabs.append(de_vocabs)\n","vocabs.append(chi_vocabs)\n","weights.append(en_weights)\n","weights.append(fr_weights)\n","weights.append(de_weights)\n","weights.append(chi_weights)"]},{"cell_type":"markdown","metadata":{"id":"8pXtx1agi4IH"},"source":["# Training and obtaining results from the Word-level translator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t38QxeaKEBeq"},"outputs":[],"source":["def index2year(idx):\n","  return 1800 + idx * 10\n","\n","def year2index(year):\n","  return (year - 1800)/10\n","\n","lang2index = {\"English\": 0, \"French\": 1, \"German\": 2, \"Mandarin\": 3}\n","\n","# word - str, input_lang - str, input_year - int, output_lang - str, output_year - int\n","def word2embedding(word, lang_idx, year_idx):\n","  return weights[lang_idx][year_idx][vocabs[lang_idx][year_idx].\n","                                             index(word)]\n","\n","def translate(word, input_lang, input_year, output_lang, output_year, vars):\n","  # Autotranslation works just for English.\n","  #   If there is autotranslation, I change the output language temporarily so that\n","  # the training file exists.\n","  autotranslation = False\n","  list_name = \"swadesh list\"\n","  if (input_lang == 'English' and output_lang == 'English'):\n","    output_lang = 'French'\n","    autotranslation = True\n","  training_file = input_lang + '-' + output_lang + \" \" + list_name\n","  \n","  lines = open(training_file).read().strip().split('\\n')\n","  print(lines)\n","\n","  input_swadesh_words = []\n","  output_swadesh_words = []\n","  for line in lines:\n","    pair = re.split(' +', line)\n","    input_swadesh_words.append(pair[0].lower())\n","    output_swadesh_words.append(pair[1].lower())\n","\n","  if autotranslation == True:\n","    output_lang = 'English'\n","    output_swadesh_words = input_swadesh_words\n","\n","  # I am training the model on each input for the given planes\n","  input_embs = []\n","  output_embs = []\n","  input_lang_idx = lang2index[input_lang]\n","  input_year_idx = int(year2index(input_year))\n","  output_lang_idx = lang2index[output_lang]\n","  output_year_idx = int(year2index(output_year))\n","\n","  for i in input_swadesh_words:\n","    print(i, end =\", \")\n","  #print(output_swadesh_words)\n","  \n","  for idx in range(len(input_swadesh_words)):\n","    train_word1 = input_swadesh_words[idx]\n","    train_word2 = output_swadesh_words[idx]\n","    # Checking if the word is in the vocabularies for the given year\n","    if vocabs[input_lang_idx][input_year_idx].count(train_word1) > 0 \\\n","    and vocabs[output_lang_idx][output_year_idx].count(train_word2) > 0:\n","      input_embs.append(word2embedding(train_word1, input_lang_idx,\n","                                       input_year_idx))\n","      output_embs.append(word2embedding(train_word2, output_lang_idx,\n","                                        output_year_idx))\n","      \n","  model = LinearRegression()\n","  model.fit(input_embs, output_embs)\n","  x_hat = word2embedding(word, input_lang_idx, input_year_idx)\n","  y_hat = model.predict(x_hat.reshape(1,-1))\n","  output_weights_all = weights[output_lang_idx][output_year_idx]\n","\n","  dist = np.linalg.norm(y_hat - output_weights_all, axis=1) # Euclidean distance\n","  indices = dist.argsort()[:vars] # Get the first \"vars\" elements sorted by minimal distance\n","  for i in indices:\n","    print(vocabs[output_lang_idx][output_year_idx][i]) # Get vector having minimum distance\n","  print(\"First argument in the following tuples is the size of the Swadesh list:\")\n","  print(np.array(input_embs).shape)\n","  print(np.array(output_embs).shape)\n","\n","# Arguments: word to translate, input language and year of it, output language\n","# and year of it, and the number of translating options\n","translate('queen', 'English', 1990, 'French', 1990, vars=10)"]},{"cell_type":"markdown","metadata":{"id":"PlfEXCa-VQg_"},"source":["# Volatility of a Word\n","\n","Code to measure distance at the closest 5 words at time t, which is subtracted from the same closest words at time t' (only in English)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1655601080106,"user":{"displayName":"Vlad Safta","userId":"06171379063017095917"},"user_tz":-60},"id":"w_XTKhBqVP4l","outputId":"7ffafba9-0320-4532-e33d-f0ba36218f3e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4003451419459667"]},"metadata":{},"execution_count":5}],"source":["def volatility(word, from_year, to_year):\n","  # Word Volatility is defined for English, which has the index 0\n","  lang_idx = 0\n","  input_year_idx = int(year2index(from_year))\n","  output_year_idx = int(year2index(to_year))\n","  input_vocab = vocabs[lang_idx][input_year_idx]\n","  output_vocab = vocabs[lang_idx][output_year_idx]\n","  input_weights = weights[lang_idx][int(year2index(from_year))]\n","  output_weights = weights[lang_idx][int(year2index(to_year))]\n","\n","  if not (input_vocab.count(word) > 0 and output_vocab.count(word) > 0):\n","    print(\"Word does not exist in the vocabularies of the given years\")\n","    return\n","\n","  word_input_embeddings = word2embedding(word, lang_idx, input_year_idx)\n","  word_output_embeddings = word2embedding(word, lang_idx, output_year_idx)\n","\n","  dists_at_input_year = np.linalg.norm(word_input_embeddings - input_weights, axis=1) # Euclidean distance\n","  indices = dists_at_input_year.argsort()[1:17] # Get the first \"vars\" elements sorted by minimal distance\n","  key_words_from_input = []\n","  key_words_input_dist = []\n","  for i in indices:\n","    curr = vocabs[lang_idx][input_year_idx][i]\n","    if output_vocab.count(curr):\n","      key_words_from_input.append(curr)\n","      key_words_input_dist.append(dists_at_input_year[i])\n","  key_words_output_dist = []\n","  for key_word in key_words_from_input:\n","    key_words_output_dist.append(np.linalg.norm(\n","          word_output_embeddings - word2embedding(key_word, lang_idx, output_year_idx)))\n","      \n","  sum = 0\n","  for idx, key_word in enumerate(key_words_from_input):\n","    sum += ((key_words_output_dist[idx] - key_words_input_dist[idx])**2)/(math.log(idx + 2, 2))\n","\n","  dists_at_output_year = np.linalg.norm(\n","      word_output_embeddings - output_weights, axis=1) # Euclidean distance\n","  indices = dists_at_output_year.argsort()[1:17] \n","  # Get the first \"vars\" elements sorted by minimal distance\n","  # The word at index 0 is the same word as the input one, so it is redundant\n","  key_words_from_output = []\n","  key_words_output_dist = []\n","  for i in indices:\n","    curr = vocabs[lang_idx][output_year_idx][i]\n","    if input_vocab.count(curr):\n","      key_words_from_output.append(curr)\n","      key_words_output_dist.append(dists_at_output_year[i])\n","  key_words_input_dist = []\n","  for key_word in key_words_from_output:\n","    key_words_input_dist.append(np.linalg.norm(\n","          word_input_embeddings - word2embedding(key_word, lang_idx, input_year_idx)))\n","\n","  for idx, key_word in enumerate(key_words_from_output):\n","    sum += ((key_words_input_dist[idx] - key_words_output_dist[idx])**2)/(math.log(idx + 2, 2))\n","\n","  return sum\n","\n","volatility(\"yesterday\", 1900, 1990)"]},{"cell_type":"markdown","metadata":{"id":"E_dCbSdZ4hVC"},"source":["Calculating the k least volatile words in a time period - optional extension"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6jvxNK54g0O"},"outputs":[],"source":["def calculate_volatilities(from_year, to_year, k):\n","  lang_idx = 0\n","  input_year_idx = int(year2index(from_year))\n","  output_year_idx = int(year2index(to_year))\n","  input_vocab = vocabs[lang_idx][input_year_idx]\n","  output_vocab = vocabs[lang_idx][output_year_idx]\n","  common_vocab = [word for word in input_vocab if word in output_vocab]\n","\n","  volatilities = [volatility(word, from_year, to_year) for word in common_vocab].cuda()\n","\n","  return volatilities, common_vocab\n","\n","volatilities, common_vocab = calculate_volatilities(1850, 1990, 68).cuda()\n","print(volatilities)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Word2WordTranslator.ipynb","provenance":[],"mount_file_id":"1U9LQ-IX8XUfRzsbPChrhmCdV66bFApSS","authorship_tag":"ABX9TyOlC6MtQmCc91fWgKtqN85+"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}