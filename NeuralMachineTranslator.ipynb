{"cells":[{"cell_type":"markdown","metadata":{"id":"REA6rTm8DOX-"},"source":["**IMPORTS**\n","\n","\n","*   Utility imports used to handle characters written in unicode, complex mathematical operations, opening files and handling NumPy arrays \n","*   Pytorch imports: autograd for differentiation, Neural Networks and also using the GPU for creating the translation models faster\n","*   Helpful \"sys\" import that permits the network to use argparse in backpropagation as a gradient clipper\n","\n","After acquiring all the imports that we need, we ensure that we are using a GPU. Note that this code *needs* a GPU in order to train models. \n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-fZjfPMCa2r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656085841734,"user_tz":-60,"elapsed":3437,"user":{"displayName":"Vlad Safta","userId":"06171379063017095917"}},"outputId":"a4a90836-65f9-456d-9e0b-3e99260a1f8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}],"source":["import unicodedata\n","import re\n","import math\n","import matplotlib.pyplot as plt\n","import psutil\n","import time\n","import datetime\n","import random\n","from random import shuffle\n","import argparse\n","import numpy as np\n","from io import open\n","import pickle\n","\n","import torch\n","from torch.autograd import Variable\n","from torch import optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.cuda\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","import sys\n","sys.argv=['']\n","del sys\n","\n","use_cuda = torch.cuda.is_available()\n","print(use_cuda)"]},{"cell_type":"markdown","source":["Create a Language class which constructs a vocabulary for the two languages from the parallel corpus. The functions are designed to add words from new sentences in the processing dataset and count them.\n","\n","We also represent each word in the language as a one-hot vector, where the value of one is present at the index of the word. So, it is also required to code appropriate functions in order to assign an index to each of the word (word2index and index2word).\n"],"metadata":{"id":"kuDtK-K8z4hm"}},{"cell_type":"code","source":["class Language:\n","  def __init__(self, language):\n","    self.name = language\n","    self.word2index = {\"SOS\": 0, \"EOS\": 1}\n","    self.word2count = {}\n","    self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","    self.n_words = 2 # considering the start-of-sentence, end-of-sentence,\n","                     # and unknown tokens as words \n","\n","  def countWords(self, word):\n","    if word in self.word2count:\n","      self.word2count[word] += 1\n","    else:\n","      self.word2count[word] = 1\n","\n","  def addSentence(self, sentence):\n","    new_sentence = ''\n","    for word in sentence.split(' '):\n","      new_word = self.addWord(word)\n","      if new_sentence:\n","        new_sentence = new_sentence + ' ' + new_word\n","      else:\n","        new_sentence = new_word\n","    return new_sentence\n","\n","  def addWord(self, word):\n","    if word not in self.word2index:\n","      self.word2index[word] = self.n_words\n","      self.index2word[self.n_words] = word\n","      self.n_words += 1\n","    return word"],"metadata":{"id":"mM0EUgxAz2zy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZNxlVyrLD7b6"},"source":["Before handling the data, we should note that we are going to use a parallel corpus file. This means that we will have a large number of pairs, where a pair is two sentences in two languages with the same meaning.\n","\n","The processes are:\n","*   Converting Unicode characters from the corpus files to ASCII characters. (code from https://stackoverflow.com/a/518232/2809427)\n","*   Simplifying the strings: converting every letter to lowercase and punctuation to end-of-sentence tags).\n","\n","In this snippet there is also an extra utility function:\n","*   Converting time measurements from seconds to hours. Essential for measuring how long does an epoch take in a human-friendly way.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEdRqfFtEW38"},"outputs":[],"source":["def uniToAscii(sentence):\n","  return ''.join(c for c in unicodedata.\n","                 normalize('NFD', sentence) if unicodedata.category(c) != 'Mn')\n","\n","def normalizeString(s):\n","  s = re.sub(r\" ##AT##-##AT## \", r\" \", s)\n","  s = uniToAscii(s.lower().strip())\n","  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","  return s\n","\n","def toHours(s):\n","  m = math.floor(s / 60)\n","  h = math.floor(m / 60)\n","  s -= m * 60\n","  m -= h * 60\n","  return '%dh %dm %ds' % (h, m, s)\n","\n","\"\"\" Maximum translation length - the resulting translation can be up to 200 \n","words \"\"\"\n","MAX_TRANSLATION_LENGTH = 200"]},{"cell_type":"markdown","metadata":{"id":"rSu6F5w9F58U"},"source":["Then, we open the file located in the given path, process the strings from each line and create the classes according to the order that we want."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewQGOG7BGDqy"},"outputs":[],"source":["def prepareLangs(language1, language2, filepath, reverse_langs):\n","  print(\"Reading lines...\")\n","\n","  # Reading lines from the file which are split into rows\n","  lines = open(filepath, encoding='utf-8').read().strip().split('\\n')\n","\n","  # Splitting each line to create the translation pair\n","  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n","\n","  if reverse_langs:\n","    pairs = [list(reversed(p)) for p in pairs]\n","    input_lang = Language(language2)\n","    output_lang = Language(language1)\n","  else:\n","    input_lang = Language(language1)\n","    output_lang = Language(language2)\n","\n","  return input_lang, output_lang, pairs"]},{"cell_type":"markdown","metadata":{"id":"MdlG04x5So5V"},"source":["The prepareData function will creates Language classes for each language and fully clean and trim the data according to the specified passed arguments. In the end, this function will return both language classes along with a set of training pairs and a set of test pairs.\n","\n","This function also provides comments related to the size of test and train translations. They are useful when creating models to judge what train proportion do we need, if there is any chance of overfitting and so on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cpb_Dn3vSqCP"},"outputs":[],"source":["def prepareData(language1, language2, filepath, reverse_langs, train_proportion):\n","    \n","  input_lang, output_lang, pairs = prepareLangs(language1, language2, \n","                                                  filepath, reverse_langs)\n","  print(\"Read %s sentence pairs\" % len(pairs))\n","\n","  for pair in pairs:\n","    for word in pair[0].split(' '):\n","      input_lang.countWords(word)\n","    for word in pair[1].split(' '):\n","      output_lang.countWords(word)\n","\n","  pairs = [(input_lang.addSentence(pair[0]),output_lang.addSentence(pair[1])) \n","            for pair in pairs]\n","\n","  shuffle(pairs)\n","    \n","  train_translations = pairs[:math.floor(len(pairs)*train_proportion)]\n","  test_translations = pairs[math.floor(len(pairs)*train_proportion):]\n","\n","  print(\"Train translation pairs: %s\" % (len(train_translations)))\n","  print(\"Test translation pairs: %s\" % (len(test_translations)))\n","  print(\"Number of words for each language:\")\n","  print(\"%s, %s -> %s\" % (input_lang.name, len(input_lang.word2count),\n","                          input_lang.n_words))\n","  print(\"%s, %s -> %s\" % (output_lang.name, len(output_lang.word2count), \n","                          output_lang.n_words))\n","  print()\n","        \n","  return input_lang, output_lang, train_translations, test_translations"]},{"cell_type":"markdown","metadata":{"id":"d6GdrEGqVMBv"},"source":["Creating the Recurrent Neural Network: Encoder and Decoder. Both components are bidirectional Long Short-Term Memory (LSTM) Neural Networks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8HJoq3uVUiP"},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","\tdef __init__(self, n_input_words, hidden_size, layers, dropout=0.1):\n","\t\tsuper(EncoderRNN, self).__init__()\n","\n","\t\tself.directions = 2\n","\t\tself.n_input_words = n_input_words\n","\t\tself.hidden_size = hidden_size\n","\t\tself.embedding = nn.Embedding(n_input_words, hidden_size)\n","\t\tself.dropout = dropout\n","\t\tself.dropout = nn.Dropout(dropout)\n","\t\tself.n_layers = layers\n","\t\tself.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n","                        num_layers=layers,dropout=dropout,\n","                        bidirectional=True,batch_first=False)\n","\t\tself.fc = nn.Linear(hidden_size*self.directions, hidden_size)\n","\n","\tdef forward(self, input_data, h_hidden, c_hidden):\n","\t\tembeddings = self.embedding(input_data)\n","\t\tembeddings = self.dropout(embeddings)\n","\t\thiddens, outputs = self.lstm(embeddings, (h_hidden, c_hidden))\n","\n","\t\treturn hiddens, outputs\n","\n","\t# Creating initial hidden states of zero for encoder corresponding to batch size\n","\tdef create_init_hiddens(self, batch_size):\n","\t\th_hidden = Variable(torch.zeros(self.n_layers*self.directions, \n","                                    batch_size, self.hidden_size))\n","\t\tc_hidden = Variable(torch.zeros(self.n_layers*self.directions, \n","                                    batch_size, self.hidden_size))\n","\t\tif use_cuda:\n","\t\t\treturn h_hidden.cuda(), c_hidden.cuda()\n","\t\telse:\n","\t\t\treturn h_hidden, c_hidden"]},{"cell_type":"markdown","source":["The implementation of the Decoder is similar to the Encoder's. It has extra tools to handle the context (the bridge between the Encoder and the Decoder) and to create scores that allows the code to show the expected translation according to the maximum value of the softmax layer."],"metadata":{"id":"t4H0UFKTyTCB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZ2jp8WqV5KS"},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","\tdef __init__(self, hidden_size, output_size, layers, dropout = 0.1):\n","\t\tsuper(DecoderRNN, self).__init__()\n","\n","\t\tself.directions = 2\n","\t\tself.output_size = output_size\n","\t\tself.hidden_size = hidden_size\n","\t\tself.n_layers = layers\n","\t\tself.dropout = dropout\n","\t\tself.embedding = nn.Embedding(output_size, hidden_size)\n","\t\tself.dropout = nn.Dropout(dropout)\n","\t\tself.score_learner = nn.Linear(hidden_size*self.directions, \n","                                   hidden_size*self.directions)\n","\t\tself.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n","                        num_layers=layers,dropout=dropout,\n","                        bidirectional=True,batch_first=False)\n","\t\tself.context_combiner = nn.Linear((hidden_size*self.directions)\n","                                      +(hidden_size*self.directions), hidden_size)\n","\t\tself.tanh = nn.Tanh()\n","\t\tself.output = nn.Linear(hidden_size, output_size)\n","\t\tself.soft = nn.Softmax(dim=1)\n","\t\tself.log_soft = nn.LogSoftmax(dim=1)\n","\n","\tdef forward(self, input_data, h_hidden, c_hidden, encoder_hiddens):\n","\n","\t\tembeddings = self.embedding(input_data)\n","\t\tembeddings = self.dropout(embeddings)\t\n","\t\tbatch_size = embeddings.shape[1]\n","\t\thiddens, outputs = self.lstm(embeddings, (h_hidden, c_hidden))\t\n","\t\ttop_hidden = outputs[0].view(self.n_layers,self.directions,\n","                                 hiddens.shape[1],\n","                                 self.hidden_size)[self.n_layers-1]\n","\t\ttop_hidden = top_hidden.permute(1,2,0).contiguous().view(batch_size,-1, 1)\n","\n","\t\tprep_scores = self.score_learner(encoder_hiddens.permute(1,0,2))\n","\t\tscores = torch.bmm(prep_scores, top_hidden)\n","\t\tattn_scores = self.soft(scores)\n","\t\tcon_mat = torch.bmm(encoder_hiddens.permute(1,2,0), attn_scores)\n","\t\th_tilde = self.tanh(self.context_combiner(torch.cat((con_mat, top_hidden),\n","\t\t                                                    dim=1).view(batch_size,-1)))\n","\t\tpred = self.output(h_tilde)\n","\t\tpred = self.log_soft(pred)\n","\n","\t\treturn pred, outputs"]},{"cell_type":"markdown","source":["Now, we go onto training the data to use the created network. First, we need to have some functions which transform sentences to input and output tensors. To do that, we firstly transform the sentence to its one-hot vector representation"],"metadata":{"id":"ZO6GpqZTOnoM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FiLVDPHwTBzo"},"outputs":[],"source":["EOS_token = 1 # end of sentence token gets index 1\n","\n","# Gets a sentence and turns it into an one-hot vector\n","def indexesFromSentence(language, sentence):\n","  return [language.word2index[word] for word in sentence.split(' ')]\n","\n","def tensorFromSentence(language, sentence):\n","  indexes = indexesFromSentence(language, sentence)\n","  indexes.append(EOS_token)\n","  if use_cuda:\n","    return torch.LongTensor(indexes).view(-1).cuda()\n","  else:\n","    return torch.LongTensor(indexes).view(-1)\n","      \n","# Converts a pair of sentence (input and target) to a pair of tensors\n","def tensorsFromPair(input_lang, output_lang, pair):\n","  input_variable = tensorFromSentence(input_lang, pair[0])\n","  target_variable = tensorFromSentence(output_lang, pair[1])\n","  return (input_variable, target_variable)"]},{"cell_type":"markdown","metadata":{"id":"IUN8xFShTxpZ"},"source":["Create a function to distribute into batches our sentence pairs to perform mini-batch gradient descent. Details of this technique can be found here: https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoO4va6vUJOD"},"outputs":[],"source":["def distribute_into_batches(data, input_lang, output_lang, batch_size, shuffle_data=True):\n","  if shuffle_data == True:\n","    shuffle(data)\n","  number_of_batches = math.floor(len(data) / batch_size)\n","  batches = list(range(number_of_batches))\n","    \n","  for batch_number in range(number_of_batches):\n","    idx = 0  \n","    input_variables = list(range(batch_size))\n","    target_variables = list(range(batch_size))    \n","    for pair in range((batch_number*batch_size),((batch_number+1)*batch_size)):\n","      input_variables[idx], target_variables[idx] = tensorsFromPair(\n","          input_lang, output_lang, data[pair])\n","      idx += 1\n","    batches[batch_number] = (input_variables, target_variables)\n","  return batches"]},{"cell_type":"markdown","metadata":{"id":"Wo-_MolzWQ5s"},"source":["The function *train_batch* performs a training loop on a single training batch. This means completing a forward pass through the model to:\n","\n","*   create a predicted translation for each sentence in the batch\n","*   computing the total loss for the batch\n","*   back-propagating on the loss to update all of the weight matrices in both the Encoder and the Decoder\n","\n","The function *train_epoch* applies the *train_batch* function iteratively for each existing training batch. Before computing the loss, the sentences lengths are equalized by padding. This means that End-of-Sentence tokens are added to the shorter sentence until the sentences are of the same length."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"deykYyKqW_Hb"},"outputs":[],"source":["def train_batch(input_batch, target_batch, encoder, decoder, \n","                encoder_optimizer, decoder_optimizer, loss_criterion):\n","\tencoder_optimizer.zero_grad()\n","\tdecoder_optimizer.zero_grad()\n","\n","\t# Create initial hidden state for encoder\n","\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(input_batch.shape[1])\n","\tenc_hiddens, enc_outputs = encoder(input_batch, enc_h_hidden, enc_c_hidden)\n","\n","\tif use_cuda:\n","\t\tdecoder_input = Variable(torch.LongTensor(1,input_batch.shape[1]).\n","\t\t                         fill_(output_lang.word2index.get(\"SOS\")).cuda())\n","\telse:\n","\t\tVariable(torch.LongTensor(1,input_batch.shape[1]).\n","                        fill_(output_lang.word2index.get(\"SOS\")))\n","\n","\tdec_h_hidden = enc_outputs[0]\n","\tdec_c_hidden = enc_outputs[1]\n","\t\n","\tloss = 0\n","\tfor i in range(target_batch.shape[0]):\n","\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, \n","                                dec_c_hidden, enc_hiddens)\n","\n","\t\tdecoder_input = target_batch[i].view(1,-1)\n","\t\tdec_h_hidden = dec_outputs[0]\n","\t\tdec_c_hidden = dec_outputs[1]\n","\t\t\n","\t\tloss += loss_criterion(pred,target_batch[i])\n","\n","\tloss.backward()\n","\n","\ttorch.nn.utils.clip_grad_norm_(encoder.parameters(),args.clip)\n","\ttorch.nn.utils.clip_grad_norm_(decoder.parameters(),args.clip)\n","\tencoder_optimizer.step()\n","\tdecoder_optimizer.step()\n","\n","\treturn loss.item() / target_batch.shape[0]\n","\n","def train_epoch(train_batches, encoder, decoder, encoder_optimizer, \n","                decoder_optimizer, loss_criterion):\n","\n","\tepoch_loss = 0\n","\tfor batch in train_batches:\n","\n","\t\tpadded_input_batch = torch.nn.utils.rnn.pad_sequence(batch[0],padding_value=EOS_token)\n","\t\tpadded_target_batch = torch.nn.utils.rnn.pad_sequence(batch[1],padding_value=EOS_token)\n","\t\n","\t\tbatch_loss = train_batch(padded_input_batch, padded_target_batch, encoder, decoder, \n","                           encoder_optimizer, decoder_optimizer, loss_criterion)\n","\t\tepoch_loss += batch_loss\n","\n","\treturn epoch_loss / len(train_batches)"]},{"cell_type":"markdown","metadata":{"id":"oYJ3Ozx8XvXS"},"source":["The function *test_batch* also creates a predicted translation for each sentence in the batch and computes the total loss for the batch, but does not do backpropagation, because testing results don't interact with weights. They are utilized only to calculate the loss on data which the model has not seen.\n","\n","Below the *test_batch* function, there is the test_epoch function which applies the *test_batch* function iteratively for each existing test batch.\n","Before computing the loss, the sentences lengths are equalized by padding. This means that End-of-Sentence tokens to the shorter sentence until the sentences are of the same length."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxAta1g8Y2AF"},"outputs":[],"source":["def test_batch(input_batch, target_batch, encoder, decoder, loss_criterion):\n","\t\n","\t# Create initial hidden state for encoder\n","\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(input_batch.shape[1])\n","\n","\tenc_hiddens, enc_outputs = encoder(input_batch, enc_h_hidden, enc_c_hidden)\n","\n","\tif use_cuda:\n","\t\tdecoder_input = Variable(torch.LongTensor(1,input_batch.shape[1]).\n","                           fill_(output_lang.word2index.get(\"SOS\")).cuda())\n","\telse: \n","\t\tVariable(torch.LongTensor(1,input_batch.shape[1]).\n","                        fill_(output_lang.word2index.get(\"SOS\")))\n","\tdec_h_hidden = enc_outputs[0]\n","\tdec_c_hidden = enc_outputs[1]\n","\t\n","\tloss = 0\n","\tfor i in range(target_batch.shape[0]):\n","\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, dec_c_hidden, enc_hiddens)\n","\n","\t\ttopv, topi = pred.topk(1,dim=1)\n","\t\tni = topi.view(1,-1)\n","\t\t\n","\t\tdecoder_input = ni\n","\t\tdec_h_hidden = dec_outputs[0]\n","\t\tdec_c_hidden = dec_outputs[1]\n","\n","\t\tloss += loss_criterion(pred,target_batch[i])\n","\t\t\n","\treturn loss.item() / target_batch.shape[0]\n","\n","def test_epoch(test_batches, encoder, decoder, loss_criterion):\n","\n","\twith torch.no_grad():\n","\t\tepoch_loss = 0\n","\t\tfor batch in test_batches:\n","\t\t\tpadded_input_batch = torch.nn.utils.rnn.pad_sequence(\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\tbatch[0], padding_value=EOS_token)\n","\t\t\tpadded_target_batch = torch.nn.utils.rnn.pad_sequence(\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\tbatch[1], padding_value=EOS_token)\n","\t\t\tbatch_loss = test_batch(padded_input_batch, padded_target_batch,\n","\t\t\t                        encoder, decoder, loss_criterion)\n","\t\t\tepoch_loss += batch_loss\n","\n","\treturn epoch_loss / len(test_batches)"]},{"cell_type":"markdown","metadata":{"id":"ihQHPHzOjHEh"},"source":["The *evaluate* function is the one that does the translation. It predicts each output word, one by one, and they are added to the expected translated string until the end-of-sentence token is predicted."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9MzJdtyj6kB"},"outputs":[],"source":["def evaluate(encoder, decoder, sentence, translate_from, translate_to, max_length):\n","\twith torch.no_grad():\n","\t\tinput_variable = tensorFromSentence(translate_from, sentence)\n","\t\tinput_variable = input_variable.view(-1,1)\n","\t\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(1)\n","\n","\t\tenc_hiddens, enc_outputs = encoder(input_variable, enc_h_hidden, enc_c_hidden)\n","\n","\t\tif use_cuda:\n","\t\t\tdecoder_input = Variable(torch.LongTensor(1,1).\n","\t\t                         fill_(translate_to.word2index.get(\"SOS\")).cuda())\n","\t\telse:\n","\t\t\tVariable(torch.LongTensor(1,1).fill_(translate_to.word2index.get(\"SOS\")))\n","\t\tdec_h_hidden = enc_outputs[0]\n","\t\tdec_c_hidden = enc_outputs[1]\n","\n","\t\tdecoded_words = []\n","\n","\t\tfor di in range(max_length):\n","\t\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, dec_c_hidden, \n","\t\t\t                            enc_hiddens)\n","\t\t\ttopv, topi = pred.topk(1,dim=1)\n","\t\t\tni = topi.item()\n","\t\t\tif ni == translate_to.word2index.get(\"EOS\"):\n","\t\t\t\tbreak\n","\t\t\telse:\n","\t\t\t\tdecoded_words.append(translate_to.index2word[ni])\n","\n","\t\t\tif use_cuda:\n","\t\t\t\tdecoder_input = Variable(torch.LongTensor(1,1).fill_(ni).cuda())\n","\t\t\telse: \n","\t\t\t\tVariable(torch.LongTensor(1,1).fill_(ni))\n","\t\t\n","\t\t\tdec_h_hidden = dec_outputs[0]\n","\t\t\tdec_c_hidden = dec_outputs[1]\n","\n","\t\toutput_sentence = ' '.join(decoded_words)\n","    \n","\t\treturn output_sentence"]},{"cell_type":"markdown","metadata":{"id":"92jpgWrqoyyQ"},"source":["The function *predict10translations* predicts translation for 10 random sentences from the test set. They show up when creating the model through the master function at every epoch. This helps to visualizing translations and evaluating them with my proper understanding. The returned output structure is the following:\n","\n","  \\> input sentence\n","\n","  \\= correct translation\n","\n","  < predicted translation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqht2Z_KowoB"},"outputs":[],"source":["def predict10translations(encoder, decoder, pairs):\n","\tfor i in range(10):\n","\t\tpair = random.choice(pairs)\n","\t\tprint('>', pair[0])\n","\t\tprint('=', pair[1])\n","\t\tprint('<', evaluate(encoder, decoder, pair[0], input_lang, output_lang,\n","\t\t                    max_length=MAX_TRANSLATION_LENGTH))\n","\t\tprint('')"]},{"cell_type":"markdown","metadata":{"id":"7_iiDYdEpmqb"},"source":["The following function completely trains the model and evaluates the progress on the train set. It uses stochastic gradient descend to do the optimization and updates the learning rate according to our learning rate hash map.\n","\n","After all the epochs are done, this function saves the whole model (not just the dictionary) for future use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uHDOS9Hp8Nm"},"outputs":[],"source":["def create_model(epochs, lr_change, lr, train_translations, \n","\t\t\t\t\t\t\t\t\t test_translations, input_lang, output_lang, batch_size, \n","\t\t\t\t\t\t\t\t\t encoder, decoder, loss_criterion):\n","\tclock_time = []\n","\tlosses = {'train set':[], 'test set': []}\n","\n","\ttest_batches = distribute_into_batches(test_translations, \n","\t                                       input_lang, output_lang, \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t batch_size, shuffle_data=False)\n","\tstart = time.time()\n"," \n","\tepoch_labels = list(range(epochs))\n","\tloss_labels = []\n","\tfor i in range(epochs):\n","\n","\t\tif i in lr_change.keys():\n","\t\t\tlr /= lr_change.get(i)\n","\t\t\n","\t\tencoder.train()\n","\t\tdecoder.train()\n","\t\tencoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n","\t\tdecoder_optimizer = optim.SGD(decoder.parameters(), lr=lr)\n","\n","\t\tbatches = distribute_into_batches(train_translations, \n","\t\t                                       input_lang, output_lang, batch_size, \n","                                           shuffle_data=True)\n","\t\ttrain_loss = train_epoch(batches, encoder, decoder, encoder_optimizer, \n","                       decoder_optimizer, loss_criterion)\n","\t\t\n","\t\tnow = time.time()\n","\t\tprint(\"Iter: %s \\nLearning Rate: %s \\nTime: %s \\nTrain Loss: %s \\n\" \n","          % (i, lr, toHours(now-start), train_loss))\n","\n","\t\ttest_loss = test_epoch(test_batches, encoder, decoder, criterion)\n","\t\tloss_labels.append(test_loss)\n","\t\tprint(\"Test set loss: %s\" % (test_loss))\n","\t\tpredict10translations(encoder, decoder, test_translations)\n","\n","\t\tclock_time.append((time.time()-start)/60)\n","\t\tlosses['train set'].append(train_loss)\n","\t\tlosses['test set'].append(test_loss)\n","\t\n","\tplt.plot(epoch_labels, loss_labels)\n","\tplt.title('Loss values at every epoch')\n","\tplt.show()\n","\t\n","\ttorch.save(encoder, input_lang_name+output_lang_name+'_enc_weights.pt')\n","\ttorch.save(decoder, input_lang_name+output_lang_name+'_dec_weights.pt')"]},{"cell_type":"markdown","metadata":{"id":"LuQ5aiuBm75L"},"source":["# Providing information for the translation:\n","*   Shortcode of the two used languages\n","*   The filepath of the dataset from which we get the translation pairs\n","*   A boolean variable which determines if we want to reverse the order of the languages in the pairs. Naturally, it is False when translating from English and True when translating to English\n","*   The proportion of the training data. Generally 0.9\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVowZTC6m-wn"},"outputs":[],"source":["input_lang_name = 'en'\n","output_lang_name = 'de'\n","\n","filepath = ('drive/MyDrive/IndividualProject/ProjectSoftwareArchive/TranslationTrainingFiles/eng-deu.txt')\n","reverse_langs = False\n","train_proportion = 0.9"]},{"cell_type":"markdown","metadata":{"id":"dxQTRGs_nNqi"},"source":["Hyperparameters:\n","*   layers: number of neural network layers in each of the Encoder and the Decoder\n","*   hidden_size: number of neurons in the hidden layers. It is the same as the input size in the LSTM. The size is fixed to 300 because the historical embeddings have the y=300.\n","*   dropout: probability of an element from a layer to be zeroed. Used to regularize the embeddings.\n","*   batch_size: number of sentences in a training/test batch\n","*   epochs: repetitions of training the given data from the corpus file\n","*   lr: Learning rate at the beginning of training the model\n","*   lr_change: Hash map representing the change of the learning rate. 3:2 means that the learning rate is divided by 2 right before training the 3rd epoch.\n","*   criterion: The criterion of loss is the negative log likelihood\n","\n","After setting the Hyperparameters, we call the prepareData function to obtain the vocabulary of the input and output language and also all the translation pairs separated to the train and test sets\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAcQgpfPnR9t"},"outputs":[],"source":["# Hyperparameters\n","batch_size = 48\n","epochs = 9\n","lr = 0.8\n","lr_change = {3:2, 5:4, 7:10}\n","\n","layers = 2\n","hidden_size = 512\n","dropout = 0.2\n","\n","criterion = nn.NLLLoss()\n","\n","# Acquiring the languages and the sets\n","input_lang, output_lang, train_translations, test_translations = prepareData(\n","    input_lang_name, output_lang_name, filepath, reverse_langs=reverse_langs, \n","    train_proportion=train_proportion)"]},{"cell_type":"markdown","source":["This snippet does the gradient clipping allowing to keep the modifications in backpropagation stable and also creates the model according to the parameters assigned above. Note that this may take up to three hours.\n","\n","In the output of this snippet, you can manually asses ten expected translations at every epoch along with both the training and test loss."],"metadata":{"id":"D8MPRkRXEKAm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"v71Hl7VBndf3"},"outputs":[],"source":["print('Train Pairs #')\n","print(len(train_translations))\n","\n","\"\"\"for gradient clipping from \n","https://github.com/pytorch/examples/blob/master/word_language_model/main.py\"\"\"\n","parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM Language Model')\n","parser.add_argument('--clip', type=float, default=0.25,\n","                    help='gradient clipping')\n","args = parser.parse_args()\n","\n","encoder = EncoderRNN(input_lang.n_words, hidden_size, layers=layers, \n","                     dropout=dropout)\n","decoder = DecoderRNN(hidden_size, output_lang.n_words, layers=layers, \n","                      dropout=dropout)\n","\n","if use_cuda:\n","\tprint('Using cuda')\n","\tencoder = encoder.cuda()\n","\tdecoder = decoder.cuda()\n","\n","create_model(epochs, lr_change, lr, train_translations, test_translations, \n","             input_lang, output_lang, batch_size, encoder, decoder, criterion)"]},{"cell_type":"markdown","source":["# Loading a saved model"],"metadata":{"id":"4w_xWlibFv58"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cL-mmcTBUsmW"},"outputs":[],"source":["encoder = torch.load('drive/MyDrive/IndividualProject/ProjectSoftwareArchive/TranslationModelFiles/French/english-german_encoder_300neurons')\n","encoder.eval()\n","decoder = torch.load('drive/MyDrive/IndividualProject/ProjectSoftwareArchive/TranslationModelFiles/French/english-german_decoder_300neurons')\n","decoder.eval()"]},{"cell_type":"markdown","source":["# Loading embedding files\n","Creating the historical language and assigning the embedding to each historical word according to the HistWords files"],"metadata":{"id":"jeWQpmwUJrE8"}},{"cell_type":"code","source":["source_vocab_path = 'drive/MyDrive/IndividualProject/en-embs/1990-vocab.pkl'\n","source_weights_path = 'drive/MyDrive/IndividualProject/en-embs/1990-w.npy'\n","source_vocab = pickle.load(open(source_vocab_path, 'rb'))\n","source_weights = np.load(source_weights_path)\n","print(len(source_vocab))\n","print(source_weights)\n","source_lang = Language('hist_en_1990')\n","for word in source_vocab:\n","  source_lang.addWord(word)\n","\n","tokens_tensor = encoder.embedding(torch.LongTensor([0,1]).cuda()).cuda()\n","hist_tensor = torch.cat((tokens_tensor.cuda(), \n","                            torch.FloatTensor(source_weights).cuda()))\n","encoder.embedding = nn.Embedding.from_pretrained(hist_tensor)\n","encoder = encoder.cuda()\n","\n","target_vocab_path = 'drive/MyDrive/IndividualProject/en-embs/1800-vocab.pkl'\n","target_weights_path = 'drive/MyDrive/IndividualProject/en-embs/1800-w.npy'\n","target_vocab = pickle.load(open(target_vocab_path, 'rb'))\n","target_weights = np.load(target_weights_path)\n","print(len(target_vocab))\n","print(target_weights)\n","target_lang = Language('hist_en_1800')\n","for word in target_vocab:\n","  target_lang.addWord(word)\n","\n","tokens_tensor = decoder.embedding(torch.LongTensor([0,1]).cuda()).cuda()\n","tensor_hist_en = torch.cat((tokens_tensor.cuda(), \n","                            torch.FloatTensor(target_weights).cuda()))\n","decoder.embedding = nn.Embedding.from_pretrained(tensor_hist_en)\n","decoder = decoder.cuda()\n","\n","print(encoder.embedding)\n","print(decoder.embedding)\n","print(target_lang.n_words)"],"metadata":{"id":"SkfnhNm3JziV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creating translation\n","\n","Please write your sentence that you want to translate. It is preferable to use punctuation."],"metadata":{"id":"roNaR5DDF2vk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mGL2KkhK2VY"},"outputs":[],"source":["translating_sentence = \"george is the king\"\n","translating_sentence = normalizeString(translating_sentence)\n","evaluate(encoder, decoder, translating_sentence, translate_from = source_lang, \n","         translate_to = target_lang, max_length=MAX_TRANSLATION_LENGTH)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"NeuralMachineTranslator.ipynb","provenance":[{"file_id":"1COCaeBPjLvEs9QyZrAQqmma39dwMCAu9","timestamp":1652782270110},{"file_id":"1_jpevMSFyFRSm1hLb6rObmRVAi3P2ye-","timestamp":1649414374620}],"machine_shape":"hm","mount_file_id":"1HO07PilLgDD3p0VMhaMwHOnwQYBPDCZ7","authorship_tag":"ABX9TyMaXAdFESQZrvsJXlnyT9ZF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}